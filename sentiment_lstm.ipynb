{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Prediction using LSTM\n",
    "In this tutorial you will implement sentiment prediction using a LSTM. Complete the code by implementing the missing parts (marked by **#TODO**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the data needs to be read in.\n",
    "\n",
    "The data originally are tweets (each tweet represented by a list of tokens), and their senitment label ('pos' or 'neg').\n",
    "\n",
    "Typically, the first step for machine learning in NLP, is to map the dataset to a matrix of size number_of_instances x size_of_representation, and to map each feature to an id (a number related with the feature).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** The first task is to create a vocabulary of the most frequent tokens, and to create a dictionary that maps each token in the vocabulary to a unique id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "\n",
    "def create_dictionary(texts, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a dictionary that maps words to ids. More frequent words have lower ids.\n",
    "    The dictionary contains at the vocab_size-1 most frequent words (and a placeholder '<unk>' for unknown words).\n",
    "    The place holder has the id 0.\n",
    "\n",
    "    :param texts: list of word lists\n",
    "    :param vocab_size\n",
    "\n",
    "    :return A dictionary that maps words to their id.\n",
    "    \"\"\"\n",
    "    counter = collections.Counter()\n",
    "    for tokens in texts:\n",
    "        counter.update(tokens)\n",
    "    vocab = [w for w,c in counter.most_common(vocab_size-1)]\n",
    "    pass # TODO: Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** Complete the function that creates lists of ids from lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ids(words, dictionary):\n",
    "    \"\"\"\n",
    "    Takes a list of words and converts them to ids using the word2id dictionary.\n",
    "    :param words: list of tokens\n",
    "    :param dictionary: maps tokens to their id\n",
    "\n",
    "    :return list of ids for each token (placeholder '<unk>' for unknown tokens)\n",
    "    \"\"\"\n",
    "    pass # TODO: Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make yourself familiar with the code below, and read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltk_data(n_texts_train=1500, n_texts_dev=500, vocab_size=10000):\n",
    "    \"\"\"\n",
    "    Reads texts from the nltk movie_reviews corpus. A word2id dictionary is \n",
    "    created and the words in the texts are substituted with their numbers. Training\n",
    "    and Development data is returned, together with labels and the word2id dictionary.\n",
    " \n",
    "    :param n_texts_train: the number of reviews that will form the training data\n",
    "    :param n_texts_dev: the number of reviews that will form the development data\n",
    "    :param vocab_size: the maximum size of the vocabulary.\n",
    "\n",
    "    :return list texts_train: A list containing lists of wordids corresponding to \n",
    "    training texts.\n",
    "    :return list texts_dev: A list containing lists of wordids corresponding to \n",
    "    development texts.\n",
    "    :return labels_train: A list containing the labels (0 or 1) for the corresponding\n",
    "    text entry in texts_train\n",
    "    :return labels_dev: A ilst containing the labels (0 or 1) for the corresponding\n",
    "    text entry in texts_dev\n",
    "    :return word2id: The dictionary obtained from the training texts that maps each\n",
    "    seen word to an id.\n",
    "    \"\"\"\n",
    "    all_ids = movie_reviews.fileids()\n",
    "    if (n_texts_train+n_texts_dev>len(all_ids)):\n",
    "        print (\"Error: There are only\",len(all_ids), \"texts in the movie_reviews corpus. Training with all of those sentences.\")\n",
    "        n_texts_train=1500\n",
    "        n_texts_dev=500\n",
    "    posids = movie_reviews.fileids('pos')\n",
    "    random.shuffle(all_ids)\n",
    "\n",
    "    texts_train=[]\n",
    "    labels_train=[]\n",
    "    texts_dev=[]\n",
    "    labels_dev=[]\n",
    "\n",
    "    for i in range(n_texts_train):\n",
    "        text = movie_reviews.raw(fileids=[all_ids[i]])\n",
    "        tokens = [word.lower() for word in word_tokenize(text)]\n",
    "        texts_train.append(tokens)\n",
    "        if all_ids[i] in posids:       \n",
    "            labels_train.append(1)\n",
    "        else:\n",
    "            labels_train.append(0)\n",
    "\n",
    "    for i in range(n_texts_train, n_texts_train+n_texts_dev):\n",
    "        text = movie_reviews.raw(fileids=[all_ids[i]])\n",
    "        tokens = [word.lower() for word in word_tokenize(text)]\n",
    "        texts_dev.append(tokens)\n",
    "        if all_ids[i] in posids:\n",
    "            labels_dev.append(1)\n",
    "        else:\n",
    "            labels_dev.append(0)\n",
    "\n",
    "    word2id=create_dictionary(texts_train, vocab_size)\n",
    "    texts_train = [to_ids(s,word2id) for s in texts_train]\n",
    "    texts_dev = [to_ids(s,word2id) for s in texts_dev]\n",
    "    return (texts_train, labels_train, texts_dev, labels_dev, word2id)\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "print('Loading data...')\n",
    "x_train, y_train, x_dev, y_dev, word2id = nltk_data(vocab_size=VOCAB_SIZE)\n",
    "print(len(x_train), 'training samples')\n",
    "print(len(x_dev), 'development samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 3 ** Now, we will train a bidirectional RNN model, and evaluate it using development data. Make yourself familiar with how the data is read in (`get_data.nltk_data(...)`). Then, complete the function `build_and_evaluate_model(...)` following the steps below.\n",
    "\n",
    " * The data we obtain from `nltk_data(...)` consists of lists of different length. Use the Keras function `pad_sequences(...)` to obtain a numpy array with `MAX_LEN` columns (longer sequences are cut off, shorter ones are padded).\n",
    " * Add the necessary layers to the model. Use the default settings if not specified otherwise.\n",
    "    * For the embedding layer, use an embedding size of 50.\n",
    "    * Use a bidirectional LSTM with 25 units (for each direction).\n",
    "    * Predict the probability for the positive class by predicting 1 value using a dense layer and the sigmoid activation.\n",
    " * Compile the model using the binary crossentropy loss (this corresponds to the log-likelihood) and the `'adam'` optimizer. Also specify that the model should use accuracy as its metric.\n",
    " * Fit the model to the training data. Pass the module variables `BATCH_SIZE` and `EPOCHS` as hyper-parameters. Also provide the development data, in order to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Conv1D\n",
    "\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "def build_and_evaluate_model(x_train, y_train, x_dev, y_dev):\n",
    "    '''Builds, trains and evaluates a keras LSTM model.'''\n",
    "    x_train = None # TODO: Exercise 3.1\n",
    "    x_dev = None # TODO: Exercise 3.1\n",
    "    model = Sequential()\n",
    "    # TODO: Ex. 3.2 - 3.4\n",
    "    # Add layers.\n",
    "    # Compile model.\n",
    "    # Fit to data.\n",
    "    # ODOT\n",
    "    score, acc = model.evaluate(x_dev, y_dev)\n",
    "    return score, acc, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now train and evaluate the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc, m = build_and_evaluate_model(x_train, y_train, x_dev, y_dev)\n",
    "print('\\ndev score:', score)\n",
    "print('dev accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can now predict the probability for the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** Time for some error analysis. \n",
    "\n",
    " * Print out the 5 tweets with the **label 1**, for which the model predicted the **smallest probabilities**.\n",
    " * Print out the 5 tweets with the **label 0**, for which the model predicted the **largest probabilities**.\n",
    "\n",
    "You can adapt the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_padded = sequence.pad_sequences(x_dev[:10], maxlen=MAX_LEN)\n",
    "prediction_dev = m.predict(x_dev_padded)\n",
    "\n",
    "id2word = {idx:word for word,idx in word2id.items()}\n",
    "\n",
    "for tweet, label, prediction in zip(x_dev_padded, y_dev, prediction_dev):\n",
    "    text = ' '.join([id2word[idx] for idx in tweet])\n",
    "    print(text, label, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
